(window.webpackJsonp=window.webpackJsonp||[]).push([[23],{375:function(t,a,e){"use strict";e.r(a);var r=e(25),i=Object(r.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"梯度下降优化算法"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#梯度下降优化算法"}},[t._v("#")]),t._v(" 梯度下降优化算法")]),t._v(" "),e("h2",{attrs:{id:""}},[e("a",{staticClass:"header-anchor",attrs:{href:"#"}},[t._v("#")]),t._v(" ...")]),t._v(" "),e("p",[t._v("回顾一下，我们有假设函数 $h$，其所有参数用 $\\theta$ 表示（$h$ is parametrized by $\\theta$），$D$ 是样例集 $\\lbrace(x^{(1)},y^{(1)}),\\cdots,(x^{(m)},y^{(m)})\\rbrace$，我们想用梯度下降来最小化经验误差（以此找到最优的 $h$，也即 $\\theta$）")]),t._v(" "),e("p",[t._v("$$ \\widehat{E}(h;D)=\\frac{1}{m}\\sum_{i=1}^m \\ell\\mathopen{}\\left(h(x^{(i)}), y^{(i)}\\right)\\mathclose{}. $$")]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"阅读材料-docsify-ignore"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#阅读材料-docsify-ignore"}},[t._v("#")]),t._v(" 阅读材料 {docsify-ignore}")]),t._v(" "),e("ul",[e("li",[t._v("https://zhuanlan.zhihu.com/p/32230623")]),t._v(" "),e("li",[t._v("https://ruder.io/optimizing-gradient-descent/index.html")]),t._v(" "),e("li",[t._v("https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c")])])])}),[],!1,null,null,null);a.default=i.exports}}]);